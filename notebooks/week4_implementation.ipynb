{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: IPO Trading Policy Optimization — Implementation\n",
    "\n",
    "This notebook documents **problem setup**, **implementation**, **validation**, and **next steps** for the IPO trading policy (REINFORCE + risk-adjusted objective).\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "- **Goal**: Maximize risk-adjusted fitness over IPO episodes via a policy \\(\\pi_\\theta\\) that chooses: participate/skip, entry day, hold days, position size.\n",
    "- **Objective**: \\(\\mathrm{Score}_\\theta = \\mathbb{E}[R_\\theta] - \\lambda \\cdot \\mathrm{CVaR}_\\alpha(R_\\theta) - \\kappa \\cdot \\mathbb{E}[C_\\theta] - \\mu \\cdot \\mathrm{MDD}_\\theta\\)\n",
    "- **Data**: Synthetic episodes (or path to rich CSV / yfinance). Each episode has a price DataFrame with `date`, `close`.\n",
    "- **Success metrics**: Objective score on train/val; CVaR and MDD; test cases passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementation\n",
    "\n",
    "**Required imports**, **objective function**, **optimization** (REINFORCE), **parameters**, and basic **logging**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required imports (run from project root)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "root = Path(\".\").resolve()\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))\n",
    "\n",
    "from src.data import Episode, generate_synthetic_prices\n",
    "from src.backtest import backtest_all, backtest_all_with_decisions\n",
    "from src.objective import score\n",
    "from src.metrics import cvar, max_drawdown\n",
    "from src.policy import PolicyParams, decide_trade\n",
    "from src.features import episodes_to_tensor\n",
    "from src.policy_network import IPOPolicyNetwork, sample_and_log_prob\n",
    "from src.train_policy import train_reinforce\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key parameters (course: hyperparameters, objective weights)\n",
    "LAM = 1.0   # CVaR penalty\n",
    "ALPHA = 0.9 # CVaR confidence level\n",
    "KAPPA = 1.0 # Cost penalty\n",
    "MU = 1.0    # MDD penalty\n",
    "COST_BPS = 10.0\n",
    "N_EPOCHS = 30\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function implementation (wraps src.objective.score)\n",
    "def compute_score(results_df, equity, lam=LAM, alpha=ALPHA, kappa=KAPPA, mu=MU):\n",
    "    \"\"\"Score = E[R] - lam*CVaR - kappa*E[Cost] - mu*MDD.\"\"\"\n",
    "    sc, metrics = score(results_df, equity, lam=lam, alpha=alpha, kappa=kappa, mu=mu)\n",
    "    return sc, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build synthetic episodes for demonstration\n",
    "def make_synthetic_episodes(n=80, N=10, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    base_date = date(2020, 1, 1)\n",
    "    episodes = []\n",
    "    for i in range(n):\n",
    "        ticker = f\"SYNTH{i:03d}\"\n",
    "        ipo_date = base_date + timedelta(days=i * 7)\n",
    "        price_df = generate_synthetic_prices(\n",
    "            ticker=ticker, ipo_date=ipo_date, N=N,\n",
    "            initial_price=float(rng.uniform(10, 100)),\n",
    "            volatility=float(rng.uniform(0.01, 0.05)), rng=rng,\n",
    "        )\n",
    "        ep = Episode(ticker=ticker, ipo_date=ipo_date, df=price_df, day0_index=0, N=N)\n",
    "        episodes.append(ep)\n",
    "    return episodes\n",
    "\n",
    "episodes = make_synthetic_episodes(80, N=10)\n",
    "print(f\"Created {len(episodes)} synthetic episodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-based baseline: fixed policy params (participate_threshold, hold_k, raw_weight)\n",
    "params_baseline = PolicyParams(participate_threshold=0.5, entry_day=0, hold_k=3, raw_weight=0.5)\n",
    "results_df, equity = backtest_all(episodes, params_baseline, cost_bps=COST_BPS)\n",
    "sc_baseline, metrics_baseline = compute_score(results_df, equity)\n",
    "print(\"Baseline (rule) score:\", round(sc_baseline, 6))\n",
    "print(\"Metrics:\", metrics_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE optimization (PyTorch)\n",
    "n_val = max(1, int(len(episodes) * 0.2))\n",
    "n_train = len(episodes) - n_val\n",
    "perm = np.random.RandomState(SEED).permutation(len(episodes))\n",
    "train_ep = [episodes[i] for i in perm[:n_train]]\n",
    "val_ep = [episodes[i] for i in perm[n_train:]]\n",
    "\n",
    "result = train_reinforce(\n",
    "    train_ep, val_episodes=val_ep,\n",
    "    n_epochs=N_EPOCHS, lr=LR, lr_schedule=\"constant\",\n",
    "    cost_bps=COST_BPS, lam=LAM, alpha=ALPHA, kappa=KAPPA, mu=MU,\n",
    "    batch_size=min(BATCH_SIZE, n_train), seed=SEED,\n",
    "    out_dir=Path(\"results\"),\n",
    ")\n",
    "print(\"Final train score:\", result[\"history\"][\"train_score\"][-1])\n",
    "if result[\"history\"][\"val_score\"]:\n",
    "    print(\"Final val score:\", result[\"history\"][\"val_score\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Validation\n",
    "\n",
    "Test cases, performance measurements, resource monitoring, edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Empty results -> score 0\n",
    "empty_df = pd.DataFrame()\n",
    "empty_equity = pd.Series(dtype=float)\n",
    "sc_empty, m_empty = score(empty_df, empty_equity)\n",
    "assert sc_empty == 0.0 and m_empty[\"score\"] == 0.0, \"Empty case should yield 0\"\n",
    "print(\"Test 1 (empty): PASS — score =\", sc_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: CVaR and MDD — constant positive returns\n",
    "constant_ret = 0.001\n",
    "n_days = 20\n",
    "equity_curve = np.cumprod([1.0] + [1 + constant_ret] * n_days)[1:]\n",
    "mdd_val = max_drawdown(pd.Series(equity_curve))\n",
    "cvar_val = cvar(np.full(10, constant_ret), alpha=0.9)\n",
    "print(\"Test 2 (constant positive ret): MDD =\", mdd_val, \"CVaR(0.9) =\", cvar_val)\n",
    "assert mdd_val == 0.0, \"No drawdown for monotonically increasing equity\"\n",
    "print(\"Test 2: PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Backtest \"never participate\" -> zero net_ret and cost (threshold=999 => no signal >= 999)\n",
    "params_skip = PolicyParams(participate_threshold=999.0, entry_day=0, hold_k=1, raw_weight=0.0)\n",
    "res_skip, eq_skip = backtest_all(episodes[:5], params_skip, cost_bps=COST_BPS)\n",
    "assert (res_skip[\"net_ret\"] == 0).all() and (res_skip[\"cost\"] == 0).all()\n",
    "print(\"Test 3 (never participate): PASS — net_ret and cost all zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance: single epoch timing (optional)\n",
    "import time\n",
    "tiny = make_synthetic_episodes(20, N=5)\n",
    "t0 = time.perf_counter()\n",
    "train_reinforce(tiny, val_episodes=tiny[:5], n_epochs=2, batch_size=10, seed=0)\n",
    "elapsed = time.perf_counter() - t0\n",
    "print(f\"Rough timing: 2 epochs on 20 episodes ≈ {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Documentation\n",
    "\n",
    "- **Decisions**: Objective uses E[R] - λ·CVaR - κ·Cost - μ·MDD (no Sharpe in code yet). REINFORCE uses mean-reward baseline and gradient clipping.\n",
    "- **Limitations**: Synthetic data only in this notebook; real data via `run_pytorch.py --data path` or yfinance. High variance with few episodes.\n",
    "- **Debug**: Run `pytest tests/test_basic.py` and `tests/test_data_*.py` for regression. Use small `n_epochs` and `batch_size` for quick checks.\n",
    "- **Next steps**: Walk-forward by year; add Sharpe term; connect to IPO index/rich CSV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
