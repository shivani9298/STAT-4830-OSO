{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: IPO Trading Policy Optimization \u2014 Implementation\n",
    "\n",
    "This notebook documents **problem setup**, **implementation**, **validation**, and **next steps** for the IPO trading policy (REINFORCE + risk-adjusted objective).\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "- **Goal**: Maximize risk-adjusted fitness over IPO episodes via a policy \\(\\pi_\\theta\\) that chooses: participate/skip, entry day, hold days, position size.\n",
    "- **Objective**: \\(\\mathrm{Score}_\\theta = \\mathbb{E}[R_\\theta] - \\lambda \\cdot \\mathrm{CVaR}_\\alpha(R_\\theta) - \\kappa \\cdot \\mathbb{E}[C_\\theta] - \\mu \\cdot \\mathrm{MDD}_\\theta\\)\n",
    "- **Data**: Synthetic episodes (or path to rich CSV / yfinance). Each episode has a price DataFrame with `date`, `close`.\n",
    "- **Success metrics**: Objective score on train/val; CVaR and MDD; test cases passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementation\n",
    "\n",
    "**Required imports**, **objective function**, **optimization** (REINFORCE), **parameters**, and basic **logging**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required imports (run from project root)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "root = Path(\".\").resolve()\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))\n",
    "\n",
    "from src.data import Episode, generate_synthetic_prices\n",
    "from src.backtest import backtest_all, backtest_all_with_decisions\n",
    "from src.objective import score\n",
    "from src.metrics import cvar, max_drawdown\n",
    "from src.policy import PolicyParams, decide_trade\n",
    "from src.features import episodes_to_tensor\n",
    "from src.policy_network import IPOPolicyNetwork, sample_and_log_prob\n",
    "from src.train_policy import train_reinforce\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key parameters (course: hyperparameters, objective weights)\n",
    "LAM = 1.0   # CVaR penalty\n",
    "ALPHA = 0.9 # CVaR confidence level\n",
    "KAPPA = 1.0 # Cost penalty\n",
    "MU = 1.0    # MDD penalty\n",
    "COST_BPS = 10.0\n",
    "N_EPOCHS = 30\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function implementation (wraps src.objective.score)\n",
    "def compute_score(results_df, equity, lam=LAM, alpha=ALPHA, kappa=KAPPA, mu=MU):\n",
    "    \"\"\"Score = E[R] - lam*CVaR - kappa*E[Cost] - mu*MDD.\"\"\"\n",
    "    sc, metrics = score(results_df, equity, lam=lam, alpha=alpha, kappa=kappa, mu=mu)\n",
    "    return sc, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build synthetic episodes for demonstration\n",
    "def make_synthetic_episodes(n=80, N=10, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    base_date = date(2020, 1, 1)\n",
    "    episodes = []\n",
    "    for i in range(n):\n",
    "        ticker = f\"SYNTH{i:03d}\"\n",
    "        ipo_date = base_date + timedelta(days=i * 7)\n",
    "        price_df = generate_synthetic_prices(\n",
    "            ticker=ticker, ipo_date=ipo_date, N=N,\n",
    "            initial_price=float(rng.uniform(10, 100)),\n",
    "            volatility=float(rng.uniform(0.01, 0.05)), rng=rng,\n",
    "        )\n",
    "        ep = Episode(ticker=ticker, ipo_date=ipo_date, df=price_df, day0_index=0, N=N)\n",
    "        episodes.append(ep)\n",
    "    return episodes\n",
    "\n",
    "episodes = make_synthetic_episodes(80, N=10)\n",
    "print(f\"Created {len(episodes)} synthetic episodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-based baseline: fixed policy params (participate_threshold, hold_k, raw_weight)\n",
    "params_baseline = PolicyParams(participate_threshold=0.5, entry_day=0, hold_k=3, raw_weight=0.5)\n",
    "results_df, equity = backtest_all(episodes, params_baseline, cost_bps=COST_BPS)\n",
    "sc_baseline, metrics_baseline = compute_score(results_df, equity)\n",
    "print(\"Baseline (rule) score:\", round(sc_baseline, 6))\n",
    "print(\"Metrics:\", metrics_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE optimization (PyTorch)\n",
    "n_val = max(1, int(len(episodes) * 0.2))\n",
    "n_train = len(episodes) - n_val\n",
    "perm = np.random.RandomState(SEED).permutation(len(episodes))\n",
    "train_ep = [episodes[i] for i in perm[:n_train]]\n",
    "val_ep = [episodes[i] for i in perm[n_train:]]\n",
    "\n",
    "result = train_reinforce(\n",
    "    train_ep, val_episodes=val_ep,\n",
    "    n_epochs=N_EPOCHS, lr=LR, lr_schedule=\"constant\",\n",
    "    cost_bps=COST_BPS, lam=LAM, alpha=ALPHA, kappa=KAPPA, mu=MU,\n",
    "    batch_size=min(BATCH_SIZE, n_train), seed=SEED,\n",
    "    out_dir=Path(\"results\"),\n",
    ")\n",
    "print(\"Final train score:\", result[\"history\"][\"train_score\"][-1])\n",
    "if result[\"history\"][\"val_score\"]:\n",
    "    print(\"Final val score:\", result[\"history\"][\"val_score\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Validation\n",
    "\n",
    "Test cases, performance measurements, resource monitoring, edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Empty results -> score 0\n",
    "empty_df = pd.DataFrame()\n",
    "empty_equity = pd.Series(dtype=float)\n",
    "sc_empty, m_empty = score(empty_df, empty_equity)\n",
    "assert sc_empty == 0.0 and m_empty[\"score\"] == 0.0, \"Empty case should yield 0\"\n",
    "print(\"Test 1 (empty): PASS \u2014 score =\", sc_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: CVaR and MDD \u2014 constant positive returns\n",
    "constant_ret = 0.001\n",
    "n_days = 20\n",
    "equity_curve = np.cumprod([1.0] + [1 + constant_ret] * n_days)[1:]\n",
    "mdd_val = max_drawdown(pd.Series(equity_curve))\n",
    "cvar_val = cvar(np.full(10, constant_ret), alpha=0.9)\n",
    "print(\"Test 2 (constant positive ret): MDD =\", mdd_val, \"CVaR(0.9) =\", cvar_val)\n",
    "assert mdd_val == 0.0, \"No drawdown for monotonically increasing equity\"\n",
    "print(\"Test 2: PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Backtest \"never participate\" -> zero net_ret and cost (threshold=999 => no signal >= 999)\n",
    "params_skip = PolicyParams(participate_threshold=999.0, entry_day=0, hold_k=1, raw_weight=0.0)\n",
    "res_skip, eq_skip = backtest_all(episodes[:5], params_skip, cost_bps=COST_BPS)\n",
    "assert (res_skip[\"net_ret\"] == 0).all() and (res_skip[\"cost\"] == 0).all()\n",
    "print(\"Test 3 (never participate): PASS \u2014 net_ret and cost all zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance: single epoch timing (optional)\n",
    "import time\n",
    "tiny = make_synthetic_episodes(20, N=5)\n",
    "t0 = time.perf_counter()\n",
    "train_reinforce(tiny, val_episodes=tiny[:5], n_epochs=2, batch_size=10, seed=0)\n",
    "elapsed = time.perf_counter() - t0\n",
    "print(f\"Rough timing: 2 epochs on 20 episodes \u2248 {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Walk-Forward (Time-Based) Validation\n\nSplit episodes by `ipo_date` (earliest 80% \u2192 train, latest 20% \u2192 test) to simulate out-of-sample evaluation. This prevents temporal leakage and is the correct validation approach for time-series data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward (time-based) validation\n",
    "# Split by ipo_date: earliest 80% \u2192 train, latest 20% \u2192 out-of-sample test\n",
    "# This avoids temporal leakage (no training on future episodes)\n",
    "\n",
    "sorted_episodes = sorted(episodes, key=lambda ep: ep.ipo_date)\n",
    "cutoff = int(len(sorted_episodes) * 0.8)\n",
    "wf_train = sorted_episodes[:cutoff]\n",
    "wf_test  = sorted_episodes[cutoff:]\n",
    "\n",
    "print(f'Walk-forward: {len(wf_train)} train episodes, {len(wf_test)} test episodes')\n",
    "print(f'  Train date range: {wf_train[0].ipo_date} \u2192 {wf_train[-1].ipo_date}')\n",
    "print(f'  Test  date range: {wf_test[0].ipo_date}  \u2192 {wf_test[-1].ipo_date}')\n",
    "\n",
    "# Train policy on the earlier cohort, evaluate on later cohort (OOS)\n",
    "wf_result = train_reinforce(\n",
    "    wf_train, val_episodes=wf_test,\n",
    "    n_epochs=N_EPOCHS, lr=LR, lr_schedule='constant',\n",
    "    cost_bps=COST_BPS, lam=LAM, alpha=ALPHA, kappa=KAPPA, mu=MU,\n",
    "    batch_size=min(BATCH_SIZE, len(wf_train)), seed=SEED,\n",
    ")\n",
    "\n",
    "wf_train_score = wf_result['history']['train_score'][-1]\n",
    "wf_oos_score   = wf_result['history']['val_score'][-1] if wf_result['history']['val_score'] else None\n",
    "\n",
    "print(f'\\nWalk-forward OOS score  : {wf_oos_score:.6f}' if wf_oos_score is not None else 'No OOS score')\n",
    "print(f'Walk-forward Train score: {wf_train_score:.6f}')\n",
    "\n",
    "# Compare vs. always-participate baseline on the test set\n",
    "params_always = PolicyParams(participate_threshold=0.0, entry_day=0, hold_k=3, raw_weight=0.5)\n",
    "res_always, eq_always = backtest_all(wf_test, params_always, cost_bps=COST_BPS)\n",
    "sc_always, _ = score(res_always, eq_always)\n",
    "print(f'Always-participate (OOS): {sc_always:.6f}')\n",
    "if wf_oos_score is not None:\n",
    "    print(f'OOS gain vs always-participate: {(wf_oos_score - sc_always):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Real-Data Pipeline\n\nThe full pipeline supports live data via `run_pytorch.py`. The notebook uses synthetic data for reproducibility and speed. To run on real S&P 500 data (from Yahoo Finance), use:\n\n```bash\n# From the project root:\npython run_pytorch.py --data yfinance --max_tickers 50 --n_epochs 20 --N 10\n```\n\nThis fetches 50 S&P 500 tickers, builds 10-day rolling episodes, and trains the policy with REINFORCE. Expected output format:\n\n```\nFetching S&P 500 constituent list...\nFetching prices from Yahoo Finance for 50 S&P 500 tickers...\nFetched 48 tickers\nTrain 384 episodes, val 96 episodes\nEpoch 1/20  loss=0.002341  train_score=-0.003412  val_score=-0.002876\n...\nEpoch 20/20 loss=0.001644  train_score= 0.000218  val_score=-0.000891\nBest epoch (by val score): 15  |  Best val score: -0.000712\n```\n\nScores near zero are expected for short windows with 10 bps costs; a negative val score with a positive or improving train score indicates the policy is learning but the test set is noisy with few episodes.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Documentation\n\n**Key design decisions:**\n- **Objective** (`src/objective.score`): `Score = E[R] - \u03bb\u00b7CVaR - \u03ba\u00b7Cost - \u03bc\u00b7MDD`. No \u03b2\u00b7Sharpe in the current code (listed as next step). Default \u03bb=\u03ba=\u03bc=1.0 puts risk and return on the same decimal scale; \u03b1=0.9 uses the worst 10% tail for CVaR.\n- **REINFORCE** (`src/train_policy.py`): Non-differentiable backtest \u2192 policy gradient. Mean-reward baseline reduces variance; gradient clipping (max norm 1.0) prevents large steps. Adam optimizer chosen for adaptive learning rate.\n- **Backtest** (`src/backtest.py`): `net_ret = weight \u00d7 excess_ret \u2212 cost`; equity curve is cumulative product of (1 + net_ret). Backtest is pure NumPy/pandas, decoupled from PyTorch.\n- **Features** (`src/features.py`): 20-dim vector: 8 price/volume base features + 12 meta features (offer price, shares, sector hash, CEO info). Missing meta filled with zeros.\n- **Validation split**: Random 80/20 in default runs; time-based (ipo_date sort) in the walk-forward section above.\n\n**Known limitations:**\n- Synthetic data only in this notebook; real data via `run_pytorch.py --data yfinance`.\n- High REINFORCE variance with few episodes; entropy bonus (coef=0.01) encourages exploration.\n- No walk-forward over multiple years yet (only one train/test split above).\n- Sharpe not in objective; regime dependency not handled.\n\n**Debug / test strategies:**\n- `pytest tests/test_basic.py` \u2014 objective, CVaR, MDD, backtest edge cases.\n- Small `n_epochs=2, batch_size=10` for quick sanity checks.\n- Check `result[\"history\"][\"train_score\"]` and `val_score` lists for convergence.\n\n**Next steps:**\n- Walk-forward over real IPO cohorts by year (2021 train \u2192 2022 test \u2192 \u2026).\n- Add \u03b2\u00b7Sharpe term to `src/objective.score()`.\n- Connect notebook to `src/dailyhistorical_21-26.csv` for actual IPO episodes.\n- Tune \u03bb, \u03ba, \u03bc via grid search on held-out cohort.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}